from django.core.management.base import BaseCommand
from django.db import transaction, connection
from pathlib import Path
import csv, re
from collections import Counter

from corpus.models import Book, Term, Posting, IndexStat

# ========= æ–‡æœ¬æ¸…æ´— =========
START_RE = re.compile(r'\*{3}\s*START OF .*?PROJECT GUTENBERG EBOOK.*?\*{3}', re.I)
END_RE   = re.compile(r'\*{3}\s*END OF .*?PROJECT GUTENBERG EBOOK.*?\*{3}', re.I)
LICENSE_RE = re.compile(r"START:\s*FULL\s*LICENSE", re.I)

# ========= è¯æŠ½å–ï¼šåªå…è®¸ a-z =========
WORD_RE = re.compile(r"[A-Za-z]+")

# ========= åœç”¨è¯ =========
STOPWORDS = {
    "a","an","and","are","as","at","be","but","by","for","if","in","into","is","it",
    "no","not","of","on","or","such","that","the","their","then","there","these",
    "they","this","to","was","will","with","so","than","too","very","can","from",
    "do","does","did","am","have","has","had",
    "he","him","his","she","her","hers","me","my","mine","you","your","yours",
    "we","our","ours","us","them","those","which","what","who","whom",
    "been","being","shall","should","would","could","may","might","must",
    "i","im","youre","youve","youll","cant","dont","didnt","wont","theres",
    "here","where","when","why","how","any","all","each","every","few",
    "more","most","other","some","only","own","same","too",
}

# ========= è¯å¹²æå–ï¼ˆPorterStemmerï¼‰=========
from nltk.stem import PorterStemmer
stemmer = PorterStemmer()


# ========= æ¸…æ´—æ–‡æœ¬ï¼šå»æ‰ Licenseã€å¼€å¤´ã€ç»“å°¾ =========
def clean_text(raw: str) -> str:
    # å…ˆåˆ‡æ‰ LICENSE
    m1 = LICENSE_RE.search(raw)
    cut = raw[:m1.start()] if m1 else raw

    # å†åˆ‡æ‰ "*** START OF ..."
    m2 = START_RE.search(cut)
    body = cut[m2.end():] if m2 else cut

    # æœ€ååˆ‡æ‰ "*** END OF ..."
    m3 = END_RE.search(body)
    body = body[:m3.start()] if m3 else body

    return body


# ========= tokenizeï¼šåˆ†è¯ + å°å†™ + åœç”¨è¯ + é•¿åº¦è¿‡æ»¤ + è¯å¹² =========
def tokenize(s: str):
    tokens = []
    for tok in WORD_RE.findall(s):
        tok = tok.lower()

        if tok in STOPWORDS:
            continue
        if len(tok) < 2:
            continue
        # å¯é€‰ï¼šè¿‡æ»¤ç‰¹åˆ«é•¿çš„ tokenï¼Œå¾ˆå¤šæ˜¯å™ªå£°
        if len(tok) > 25:
            continue

        tok = stemmer.stem(tok)
        tokens.append(tok)

    return tokens


# ========= ä¸»å‘½ä»¤ =========
class Command(BaseCommand):
    help = "æ„å»ºå€’æ’ç´¢å¼•ï¼ˆæ­£ç¡® DF é¡ºåºï¼ŒTopK + é«˜é¢‘/ä½é¢‘è¿‡æ»¤ï¼‰"

    def add_arguments(self, parser):
        parser.add_argument("--meta", default="../selected_meta.csv")
        parser.add_argument("--dir", default="../books_html_kept")
        parser.add_argument("--limit", type=int, default=0)
        parser.add_argument("--topk", type=int, default=3000)      # æ¯æœ¬ä¹¦æœ€å¤šå– topK è¯
        parser.add_argument("--batch-size", type=int, default=5000)

    def handle(self, *args, **opts):

        meta_csv = Path(opts["meta"]).resolve()
        book_dir = Path(opts["dir"]).resolve()
        rows = list(csv.DictReader(meta_csv.open(encoding="utf8", errors="ignore")))

        if opts["limit"] > 0:
            rows = rows[:opts["limit"]]

        topk  = opts["topk"]
        bsize = opts["batch_size"]

        posting_buf = []
        term_cache = {}   # term_str -> term_id

        total_docs = 0
        total_len  = 0

        self.stdout.write("ğŸš€ å¼€å§‹æ„å»ºå€’æ’ç´¢å¼•...")

        with transaction.atomic():

            # ===== 0. æ¸…ç©ºæ—§æ•°æ®ï¼ˆåªåœ¨å•ç‹¬ç´¢å¼•åº“ä¸­è¿™ä¹ˆå¹²ï¼‰=====
            Posting.objects.all().delete()
            Term.objects.all().delete()

            # ===== 1. éå†ä¹¦ç±ï¼šæ¸…æ´— + tokenize + Counter + TopK + å†™ Posting =====
            for i, row in enumerate(rows, 1):

                tid_str = (row.get("Text#") or "").strip()
                if not tid_str.isdigit():
                    continue
                tid = int(tid_str)

                p = book_dir / f"{tid}.txt"
                if not p.exists():
                    continue

                raw = p.read_text(encoding="utf8", errors="ignore")
                toks = tokenize(clean_text(raw))

                if not toks:
                    continue

                book, _ = Book.objects.update_or_create(
                    text_id=tid,
                    defaults=dict(
                        title=row.get("Title", ""),
                        authors=row.get("Authors", ""),
                        local_path=f"books_html_kept/{tid}.txt",
                        doc_len_tokens=len(toks),
                    )
                )

                total_docs += 1
                total_len  += len(toks)

                counter = Counter(toks)

                # TopK é«˜é¢‘è¯ï¼ˆtopk <=0 è¡¨ç¤ºä¸ç”¨æˆªæ–­ï¼‰
                if topk and topk > 0:
                    items = counter.most_common(topk)
                else:
                    items = counter.items()

                for term_str, tf in items:
                    if term_str not in term_cache:
                        t = Term.objects.create(term=term_str, df=0)
                        term_cache[term_str] = t.id

                    posting_buf.append(Posting(
                        term_id=term_cache[term_str],
                        book=book,
                        tf=tf,
                    ))

                    if len(posting_buf) >= bsize:
                        Posting.objects.bulk_create(posting_buf, batch_size=bsize)
                        posting_buf = []

                if i % 50 == 0:
                    self.stdout.write(f"â€¦ å·²å¤„ç† {i} æœ¬ä¹¦")

            if posting_buf:
                Posting.objects.bulk_create(posting_buf, batch_size=bsize)

            if total_docs == 0:
                self.stdout.write(self.style.WARNING("âš  æ²¡æœ‰æˆåŠŸå¤„ç†ä»»ä½•ä¹¦ï¼Œç»“æŸã€‚"))
                return

            # ===== 2. æ­£ç¡®è®¡ç®— DFï¼ˆå”¯ä¸€æ¥æº = Postingï¼‰=====
            self.stdout.write("ğŸ” é‡æ–°è®¡ç®— DF ...")
            for term in Term.objects.all():
                df = (
                    Posting.objects
                    .filter(term_id=term.id)
                    .values("book_id")
                    .distinct()
                    .count()
                )
                term.df = df
                term.save()

            N = total_docs

            # ===== 3. DF è¿‡æ»¤ï¼šé«˜é¢‘ + ä½é¢‘ =====
            self.stdout.write("ğŸ§¹ æŒ‰ DF åšé«˜é¢‘ / ä½é¢‘ æ¸…ç† ...")

            # é«˜é¢‘ï¼šDF >= 95% æ–‡æ¡£ï¼ˆthe, and, of...ï¼‰
            high_cut = int(N * 0.95)
            if high_cut > 0:
                Term.objects.filter(df__gte=high_cut).delete()

            # ä½é¢‘ï¼šDF <= 2ï¼ˆå™ªå£°ã€æ‹¼å†™é”™è¯¯ã€äººåç­‰ï¼‰
            Term.objects.filter(df__lte=2).delete()

            # ===== 4. åˆ é™¤å­¤ç«‹ Posting =====
            self.stdout.write("ğŸ§¹ æ¸…ç†å­¤ç«‹ postings ...")
            with connection.cursor() as cursor:
                cursor.execute(
                    "DELETE FROM postings WHERE term_id NOT IN (SELECT id FROM terms);"
                )

            # ===== 5. æœ€ç»ˆå†ç®—ä¸€æ¬¡ DFï¼Œä¿è¯ä¸€è‡´ =====
            self.stdout.write("ğŸ“ æœ€ç»ˆ DF æ ¡æ­£ ...")
            for term in Term.objects.all():
                df = (
                    Posting.objects
                    .filter(term_id=term.id)
                    .values("book_id")
                    .distinct()
                    .count()
                )
                term.df = df
                term.save()

            # ===== 6. ç»Ÿè®¡ä¿¡æ¯ =====
            avg_len = total_len / total_docs
            IndexStat.objects.update_or_create(
                key="N_docs", defaults={"value": str(total_docs)}
            )
            IndexStat.objects.update_or_create(
                key="avg_doc_len", defaults={"value": str(avg_len)}
            )

        self.stdout.write(self.style.SUCCESS("ğŸ‰ å€’æ’ç´¢å¼•æ„å»ºå®Œæˆ"))
        self.stdout.write(f"ğŸ“Š æ–‡æ¡£æ•°: {total_docs}, å¹³å‡é•¿åº¦: {avg_len:.2f} tokens")
        self.stdout.write(f"ğŸ“š è¯å…¸å¤§å°: {Term.objects.count()} ä¸ª term")
