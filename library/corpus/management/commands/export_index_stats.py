from django.core.management.base import BaseCommand
from corpus.models import Book, Term, Posting, IndexStat
from collections import defaultdict
from pathlib import Path
import csv


class Command(BaseCommand):
    help = "å¯¼å‡º TFâ€“IDF ä¸å€’æ’ç´¢å¼•æµ‹è¯•æ•°æ®ï¼ˆè¯å…¸è§„æ¨¡ã€posting æ•°ã€ç¨€ç–æ€§ç­‰ï¼‰"

    def handle(self, *args, **opts):

        # è¾“å‡ºæ–‡ä»¶è·¯å¾„
        out_global = Path("test/index_global_stats.csv")
        out_books  = Path("test/index_book_stats.csv")
        out_vocab  = Path("test/vocab_stats.csv")

        self.stdout.write("ğŸ“Š å¯¼å‡º TFâ€“IDF ä¸å€’æ’ç´¢å¼•ç»Ÿè®¡æ•°æ®...")

        # ===============================
        # 1. å…¨å±€ç»Ÿè®¡
        # ===============================

        try:
            N_docs = int(IndexStat.objects.get(key="N_docs").value)
        except:
            N_docs = Book.objects.count()

        try:
            avg_doc_len = float(IndexStat.objects.get(key="avg_doc_len").value)
        except:
            if N_docs > 0:
                total_len = sum(Book.objects.values_list("doc_len_tokens", flat=True))
                avg_doc_len = total_len / N_docs
            else:
                avg_doc_len = 0

        vocab_size = Term.objects.count()
        posting_total = Posting.objects.count()

        # å†™å…¥ index_global_stats.csv
        with out_global.open("w", newline="", encoding="utf8") as f:
            w = csv.writer(f)
            w.writerow(["metric", "value"])
            w.writerow(["N_docs", N_docs])
            w.writerow(["avg_doc_len", avg_doc_len])
            w.writerow(["vocab_size_after_filter", vocab_size])
            w.writerow(["posting_total", posting_total])

        self.stdout.write(f"âœ… å…¨å±€ç´¢å¼•ç»Ÿè®¡ â†’ {out_global}")

        # ===============================
        # 2. æ¯æœ¬ä¹¦çš„ TFâ€“IDF ç¨€ç–åº¦ç­‰
        # ===============================
        with out_books.open("w", newline="", encoding="utf8") as f:
            w = csv.writer(f)
            w.writerow([
                "book_id",
                "token_count",
                "tfidf_nonzero",
                "unique_terms",
                "sparsity_percent"
            ])

            for book in Book.objects.all():
                toks = book.doc_len_tokens
                postings = Posting.objects.filter(book=book)

                tfidf_nonzero = postings.count()
                unique_terms = len({p.term_id for p in postings})

                if vocab_size > 0:
                    sparsity = tfidf_nonzero / vocab_size * 100
                else:
                    sparsity = 0

                w.writerow([
                    book.text_id,
                    toks,
                    tfidf_nonzero,
                    unique_terms,
                    f"{sparsity:.3f}"
                ])

        self.stdout.write(f"âœ… æ¯æœ¬ä¹¦ç»Ÿè®¡å·²å†™å…¥ â†’ {out_books}")

        # ===============================
        # 3. è¯å…¸ç»Ÿè®¡ï¼ˆæ¯ä¸ª term çš„ dfï¼‰
        # ===============================
        with out_vocab.open("w", newline="", encoding="utf8") as f:
            w = csv.writer(f)
            w.writerow(["term_id", "term", "df"])

            for t in Term.objects.all():
                w.writerow([t.id, t.term, t.df])

        self.stdout.write(f"âœ… è¯å…¸ç»Ÿè®¡å·²å†™å…¥ â†’ {out_vocab}")

        self.stdout.write(self.style.SUCCESS("ğŸ‰ æ‰€æœ‰ CSV æ–‡ä»¶ç”Ÿæˆå®Œæ¯•ï¼"))
